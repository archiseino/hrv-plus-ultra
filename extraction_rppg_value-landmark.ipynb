{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction rPPG Value\n",
    "\n",
    "---\n",
    "\n",
    "This file is for the UbBFC-Phys dataset extraction.\n",
    "\n",
    "Powered by the rPPG toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Dependencies\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "## Import the rPPG methods\n",
    "from methods.POS import POS_WANG\n",
    "from methods.LGI import LGI\n",
    "from methods.GREEN import GREEN\n",
    "from methods.CHROM import CHROME_DEHAAN\n",
    "from methods.OMIT import OMIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model=\"Models/blaze_face_short_range.tflite\"\n",
    "base_model=\"Mediapipe-Models/face_landmarker.task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Core method POS \n",
    "def POS(signal, **kargs):\n",
    "    \"\"\"\n",
    "    POS method on CPU using Numpy.\n",
    "\n",
    "    The dictionary parameters are: {'fps':float}.\n",
    "\n",
    "    Wang, W., den Brinker, A. C., Stuijk, S., & de Haan, G. (2016). Algorithmic principles of remote PPG. IEEE Transactions on Biomedical Engineering, 64(7), 1479-1491. \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    eps: A small constant (10^-9) used to prevent division by zero in normalization steps.\n",
    "    X: The input signal, which is a 3D array where:\n",
    "    e: Number of estimators or regions in the frame (like different parts of the face).\n",
    "    c: Color channels (3 for RGB).\n",
    "    f: Number of frames.\n",
    "    w: Window length, determined by the camera's frame rate (fps). For example, at 20 fps, w would be 32 (which corresponds to about 1.6 seconds of video).\n",
    "    \"\"\"\n",
    "    eps = 10**-9\n",
    "    X = signal\n",
    "    e, c, f = X.shape # Number of estimators, color channels, and frames\n",
    "    w = int(1.6 * kargs['fps']) # Window length in frames\n",
    "\n",
    "    \"\"\"\n",
    "    P: A fixed 2x3 matrix used for the projection step. It defines how to transform the color channels (RGB) into a new space.\n",
    "    Q: This is a stack of the matrix P repeated e times, where each P corresponds to an estimator (region of interest) in the video.\n",
    "    \"\"\"\n",
    "    P = np.array([[0, 1, -1], [-2, 1, 1]]) ## Pulse Rate Matricies\n",
    "    Q = np.stack([P for _ in range(e)], axis = 0)\n",
    "\n",
    "    \"\"\"\n",
    "    H: A matrix to store the estimated heart rate signal over time for each estimator.\n",
    "    n: The current frame in the sliding window.\n",
    "    m: The start index of the sliding window (calculating which frames are part of the current window).\n",
    "    \"\"\"\n",
    "    H = np.zeros((e, f))\n",
    "    for n in np.arange(w, f):\n",
    "        # Start index of sliding window \n",
    "        m = n - w + 1\n",
    "\n",
    "        \"\"\"\n",
    "        Temporal Normalization (Equation 5 from the paper): This step ensures that the signal is invariant to global lighting changes and other noise factors.\n",
    "        \"\"\"\n",
    "        Cn = X[:, :, m:(n+1)]\n",
    "        M = 1.0 / (np.mean(Cn, axis = 2) + eps)\n",
    "        M = np.expand_dims(M, axis=2) # shape [e, c, w]\n",
    "        Cn = np.multiply(Cn, M)\n",
    "\n",
    "        \"\"\"\n",
    "        Projection (Equation 6 from the paper): This step transforms the RGB values into a space where the signal from blood flow (heart rate) is more distinct.\n",
    "        \"\"\"\n",
    "        S = np.dot(Q, Cn)\n",
    "        S = S[0, :, :, :]\n",
    "        S = np.swapaxes(S, 0, 1) \n",
    "\n",
    "        \"\"\"\n",
    "        Tuning (Equation 7 from the paper): This step adjusts the projected components to make the heart rate signal clearer.\n",
    "        \"\"\"\n",
    "        S1 = S[:, 0, :]\n",
    "        S2 = S[:, 1, :]\n",
    "        alpha = np.std(S1, axis=1) / (eps + np.std(S2, axis=1))\n",
    "        alpha - np.expand_dims(alpha, axis=1)\n",
    "        Hn = np.add(S1, alpha * S2)\n",
    "        Hnm = Hn - np.expand_dims(np.mean(Hn, axis=1), axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        Overlap-Adding (Equation 8 from the paper): This step combines the processed signals from each frame to form the final output heart rate signal.\n",
    "        \"\"\"\n",
    "        H[:, m:(n + 1)] = np.add(H[:, m:(n + 1)], Hnm)  # Add the tuned signal to the output matrix\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Landmarker Setup\n",
    "\n",
    "Props\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Facelandmarker Object\n",
    "base_options = python.BaseOptions(model_asset_path=base_model)\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    num_faces=1,\n",
    "    running_mode = VisionRunningMode.IMAGE,\n",
    ")\n",
    "landmarker = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cheek_rois(landmarks, image_shape):\n",
    "    h, w, _ = image_shape\n",
    "    left_cheek_indices = [111, 121, 50, 142]\n",
    "    right_cheek_indices = [350, 340, 355, 280]\n",
    "\n",
    "    left_cheek_points = [(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) for idx in left_cheek_indices]\n",
    "    right_cheek_points = [(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) for idx in right_cheek_indices]\n",
    "\n",
    "    left_cheek_rect = (\n",
    "        min([pt[0] for pt in left_cheek_points]), min([pt[1] for pt in left_cheek_points]),\n",
    "        max([pt[0] for pt in left_cheek_points]), max([pt[1] for pt in left_cheek_points])\n",
    "    )\n",
    "    # print(\"Left Cheek Rect:\", left_cheek_rect)\n",
    "    right_cheek_rect = (\n",
    "        min([pt[0] for pt in right_cheek_points]), min([pt[1] for pt in right_cheek_points]),\n",
    "        max([pt[0] for pt in right_cheek_points]), max([pt[1] for pt in right_cheek_points])\n",
    "    )\n",
    "    # print(\"Right Cheek Rect:\", right_cheek_rect)\n",
    "\n",
    "    return left_cheek_rect, right_cheek_rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rgb_from_rect(rect, image):\n",
    "    x_min, y_min, x_max, y_max = rect\n",
    "    roi = image[y_min:y_max, x_min:x_max]\n",
    "    return roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detector Setup\n",
    "\n",
    "Props setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create faceDetector Object\n",
    "# base_options = python.BaseOptions(model_asset_path=base_model)\n",
    "# FaceDetector = mp.tasks.vision.FaceDetector\n",
    "# FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "# VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "# options = FaceDetectorOptions(\n",
    "#     base_options=base_options,\n",
    "#     running_mode = VisionRunningMode.IMAGE,\n",
    "# )\n",
    "# detector = vision.FaceDetector.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Fundamental cardiac frequency (fâ‚€): This is the primary component that directly corresponds to the heart rate. It typically falls between 0.5-3 Hz (30-180 BPM) depending on age, fitness level, and physiological state. This is the dominant frequency in a PPG spectrum and represents the actual heartbeat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation-Stuff\n",
    "\n",
    "Ayaya\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Automation Setups\n",
    "\n",
    "    Variables to store the RGB signals from the left and right cheeks,\n",
    "    as well as the combined signal from both cheeks.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Utils\n",
    "fs = 35 # Sampling rate in Hz for RPPG signal\n",
    "\n",
    "# Fixed adjustment values (in pixels)\n",
    "margin_x = 10  # Adjust horizontally\n",
    "scaling_factor = 0.8\n",
    "\n",
    "## Make the cropped_face_frames global\n",
    "cropped_face_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Since the base folders path is around \"datasets/subjects, we will create a target folders processed\n",
    "\"\"\"\n",
    "def extract_frames_and_save_rppg(video_path, output_dir, method_list, subject, task, fs=35, margin_x=10, scaling_factor=0.8):\n",
    "\n",
    "    video_file = cv2.VideoCapture(video_path)\n",
    "    left_r_signal, left_g_signal, left_b_signal = [], [], []\n",
    "    right_r_signal, right_g_signal, right_b_signal = [], [], []\n",
    "    combined_b_signal, combined_g_signal, combined_r_signal = [], [], []\n",
    "\n",
    "    while video_file.isOpened():\n",
    "        ret, frame = video_file.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mp_image = mp.Image(\n",
    "            image_format=mp.ImageFormat.SRGB,\n",
    "            data=image_rgb\n",
    "        )\n",
    "\n",
    "        # Get the landmarks\n",
    "        result = landmarker.detect(mp_image)\n",
    "\n",
    "        if result.face_landmarks:\n",
    "            for face_landmark in result.face_landmarks:\n",
    "\n",
    "                # Get cheek ROIs\n",
    "                left_cheek_rect, right_cheek_rect = get_cheek_rois(face_landmark, frame.shape)\n",
    "\n",
    "                # Draw both cheek ROIs with rectangles\n",
    "                cv2.rectangle(frame, (left_cheek_rect[0], left_cheek_rect[1]), (left_cheek_rect[2], left_cheek_rect[3]), (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (right_cheek_rect[0], right_cheek_rect[1]), (right_cheek_rect[2], right_cheek_rect[3]), (0, 255, 0), 2)\n",
    "\n",
    "                # Extract the left and right cheek ROIs\n",
    "                left_cheek_roi = extract_rgb_from_rect(left_cheek_rect, frame)\n",
    "                right_cheek_roi =extract_rgb_from_rect(right_cheek_rect, frame)\n",
    "\n",
    "                # Calculate mean pixel values for the RGB channels\n",
    "                left_cheek_rgb = cv2.mean(left_cheek_roi)[:3]\n",
    "                right_cheek_rgb = cv2.mean(right_cheek_roi)[:3]\n",
    "\n",
    "                # Append the RGB values to the respective lists\n",
    "                left_r_signal.append(left_cheek_rgb[0])\n",
    "                left_g_signal.append(left_cheek_rgb[1])\n",
    "                left_b_signal.append(left_cheek_rgb[2])\n",
    "\n",
    "                right_r_signal.append(right_cheek_rgb[0])\n",
    "                right_g_signal.append(right_cheek_rgb[1])\n",
    "                right_b_signal.append(right_cheek_rgb[2])\n",
    "\n",
    "                # Combine and average the RGB values from both cheeks\n",
    "                combined_r = (left_cheek_rgb[0] + right_cheek_rgb[0]) / 2\n",
    "                combined_g = (left_cheek_rgb[1] + right_cheek_rgb[1]) / 2\n",
    "                combined_b = (left_cheek_rgb[2] + right_cheek_rgb[2]) / 2\n",
    "\n",
    "                # Append the combined RGB values to the respective lists\n",
    "                combined_r_signal.append(combined_r)\n",
    "                combined_g_signal.append(combined_g)\n",
    "                combined_b_signal.append(combined_b)\n",
    "\n",
    "        ## Convert back to BGR\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    ## Close the video file\n",
    "    video_file.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    ## Plot the RGB signals in one line\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(combined_g_signal, label='Green Channel', color='green')\n",
    "    plt.plot(combined_r_signal, label='Red Channel', color='red')\n",
    "    plt.plot(combined_b_signal, label='Blue Channel', color='blue')\n",
    "    plt.title('Combined RGB Signals from Cheeks')\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.ylabel('Pixel Intensity')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    ## Process the RGB signals\n",
    "    rgb_signal = np.array([combined_r_signal, combined_g_signal, combined_b_signal])\n",
    "    rgb_signal = rgb_signal.reshape((1, 3, -1))  \n",
    "    rppg_signal = POS(rgb_signal, fps=fs)\n",
    "    rppg_signal = rppg_signal.reshape(-1)  \n",
    "\n",
    "    ## Plot the RPPG signal\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(rppg_signal, label='RPPG Signal', color='blue')\n",
    "    plt.title('RPPG Signal from Combined Cheek RGB Signals')\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    ## Filtering\n",
    "    filtered_signal = preprocess_ppg(rppg_signal, fs)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(filtered_signal, label='Filtered RPPG Signal', color='blue')\n",
    "    plt.title('Filtered RPPG Signal from Combined Cheek RGB Signals')\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ppg(signal, fs = 35):\n",
    "    \"\"\" Computes the Preprocessed PPG Signal, this steps include the following:\n",
    "        1. Moving Average Smoothing\n",
    "        2. Bandpass Filtering\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        signal (numpy array): \n",
    "            The PPG Signal to be preprocessed\n",
    "        fs (float): \n",
    "            The Sampling Frequency of the Signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array: \n",
    "            The Preprocessed PPG Signal\n",
    "    \n",
    "    \"\"\" \n",
    "\n",
    "    # 2. Bandpass filter to isolate the cardiac component (0.4-2.5 Hz)\n",
    "    b_bp, a_bp = scipy.signal.butter(3, [0.7, 2.5], btype='band', fs=fs)\n",
    "    filtered = scipy.signal.filtfilt(b_bp, a_bp, signal)\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject s51 not found, skipping\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"s51\",]\n",
    "tasks = [\"T1\", \"T3\"]\n",
    "methods = [\"POS\"]\n",
    "\n",
    "for subject in subjects:\n",
    "    if not os.path.exists(f\"{subject}\"):\n",
    "        print(f\"Subject {subject} not found, skipping\")\n",
    "    \n",
    "    for task in tasks:\n",
    "\n",
    "        video_path = f\"UBFC-Phys/{subject}/vid_{subject}_{task}.avi\"\n",
    "        if os.path.exists(video_path):\n",
    "            extract_frames_and_save_rppg(\n",
    "                video_path=video_path,\n",
    "                output_dir=f\"./{subject}\",\n",
    "                method_list=methods,\n",
    "                subject=subject,\n",
    "                task=task,\n",
    "                fs=35\n",
    "            )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## method list\n",
    "# method_list = [\"POS\", \"LGI\", \"GREEN\", \"CHROM\", \"OMIT\"]\n",
    "# # Output directory\n",
    "# output_dir = \"./s51\"\n",
    "\n",
    "\n",
    "# # Convert list to NumPy array if needed (rPPG methods expect list of frames)\n",
    "# print(f\"Extracted {len(cropped_face_frames)} frames for {subject}-{task}\")\n",
    "\n",
    "# # Process using each rPPG method\n",
    "# for method_name in method_list:\n",
    "#     if method_name == \"POS\":\n",
    "#         signal = POS_WANG(cropped_face_frames, fs=35)\n",
    "#     elif method_name == \"LGI\":\n",
    "#         signal = LGI(cropped_face_frames)\n",
    "#     elif method_name == \"GREEN\":\n",
    "#         signal = GREEN(cropped_face_frames)\n",
    "#     elif method_name == \"CHROM\":\n",
    "#         signal = CHROM(cropped_face_frames, FS=35)\n",
    "#     elif method_name == \"OMIT\":\n",
    "#         signal = OMIT(cropped_face_frames,)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown method: {method_name}\")\n",
    "\n",
    "#     signal = signal.reshape(-1)\n",
    "\n",
    "\n",
    "#     save_path = os.path.join(output_dir, f\"{subject}_{task}_{method_name}_rppg.npy\")\n",
    "#     np.save(save_path, signal)\n",
    "#     print(f\"Saved {method_name} signal to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph of the POS, LGI and OMIT methods\n",
    "\n",
    "For graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject s51 not found, skipping\n"
     ]
    }
   ],
   "source": [
    "## Looping the s1 subject npy files\n",
    "subjects = [\"s51\"]\n",
    "\n",
    "## Get the .npy files of the POS, LGI, and OMIT methods\n",
    "for subject in subjects:\n",
    "    if not os.path.exists(f\"{subject}\"):\n",
    "        print(f\"Subject {subject} not found, skipping\")\n",
    "    \n",
    "    for task in tasks:\n",
    "        ## Plot the path of the OMIT, POS, and LGI, CHROM and GREEN\n",
    "        pos_path = os.path.join(subject, f\"{subject}_{task}_POS_rppg.npy\")\n",
    "        lgi_path = os.path.join(subject, f\"{subject}_{task}_LGI_rppg.npy\")\n",
    "        omit_path = os.path.join(subject, f\"{subject}_{task}_OMIT_rppg.npy\")\n",
    "        green_path = os.path.join(subject, f\"{subject}_{task}_GREEN_rppg.npy\")\n",
    "        chrom_path = os.path.join(subject, f\"{subject}_{task}_CHROM_rppg.npy\")\n",
    "\n",
    "        if os.path.exists(pos_path) and os.path.exists(lgi_path) and os.path.exists(omit_path) and os.path.exists(green_path) and os.path.exists(chrom_path):\n",
    "            pos_signal = np.load(pos_path)\n",
    "            lgi_signal = np.load(lgi_path)\n",
    "            omit_signal = np.load(omit_path)\n",
    "            green_signal = np.load(green_path)\n",
    "            chrom_signal = np.load(chrom_path)\n",
    "\n",
    "            ## Plot one by one\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            plt.subplot(3, 2, 1)\n",
    "            plt.plot(pos_signal, label=\"POS\")\n",
    "            plt.title(f\"{subject} {task} POS\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 2, 2)\n",
    "            plt.plot(lgi_signal, label=\"LGI\")\n",
    "            plt.title(f\"{subject} {task} LGI\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(3, 2, 3)\n",
    "            plt.plot(omit_signal, label=\"OMIT\")\n",
    "            plt.title(f\"{subject} {task} OMIT\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 2, 4)\n",
    "            plt.plot(green_signal, label=\"GREEN\")\n",
    "            plt.title(f\"{subject} {task} GREEN\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.legend()    \n",
    "\n",
    "            plt.subplot(3, 2, 5)\n",
    "            plt.plot(chrom_signal, label=\"CHROM\")\n",
    "            plt.title(f\"{subject} {task} CHROM\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv\n",
    "Stuff to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_signal(rppg_signal, fs):\n",
    "#     # Narrower bandpass around expected heart rate frequencies (0.8-2.0 Hz)\n",
    "#     b, a = signal.butter(3, [0.8, 2.0], btype='band', fs=fs)\n",
    "#     filtered = signal.filtfilt(b, a, rppg_signal)\n",
    "    \n",
    "#     # Additional lowpass to remove high-frequency noise\n",
    "#     b2, a2 = signal.butter(3, 2.5, btype='low', fs=fs)\n",
    "#     filtered = signal.filtfilt(b2, a2, filtered)\n",
    "    \n",
    "#     # Moving average smoothing\n",
    "#     window = int(fs * 0.15)  # 150ms window\n",
    "#     smoothed = np.convolve(filtered, np.ones(window)/window, mode='same')\n",
    "    \n",
    "#     return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def processing_signal(subject_path, task, fs=35):\n",
    "    \n",
    "#     ## Load the RPPG Signal\n",
    "#     rppg_path = subject_path + f\"/{task}-rppg.npy\"\n",
    "#     rppg_signal = np.load(rppg_path)\n",
    "\n",
    "#     ## Compute the Frequency Component (Before)\n",
    "#     # Perform FFT on the RPPG signal\n",
    "#     fft_result = np.fft.fft(rppg_signal)\n",
    "#     fft_freqs = np.fft.fftfreq(len(rppg_signal), d=1/fs)\n",
    "\n",
    "#     # Take the magnitude of the FFT result\n",
    "#     fft_magnitude = np.abs(fft_result)\n",
    "\n",
    "#     # Plot the FFT result\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(fft_freqs[:len(fft_freqs)//2], fft_magnitude[:len(fft_magnitude)//2], color='blue')\n",
    "#     plt.title('Frequency Spectrum of the RPPG Signal')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Magnitude')\n",
    "#     plt.grid(True)\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(subject_path + f\"/{task}-fft-before.png\")\n",
    "#     plt.close()\n",
    "\n",
    "#     ## Doing the preprocessing\n",
    "#     ## Filter RPPG Signal\n",
    "#     preprocessed_signal = preprocess_ppg(rppg_signal, fs=35)\n",
    "\n",
    "#     ## Show the Filtered RPPG Signal\n",
    "#     fig, ax = plt.subplots(2, 1, figsize=(20, 6))\n",
    "#     ax[0].plot(rppg_signal, color='black')\n",
    "#     ax[0].set_title('rPPG Signal - Before Filtering')\n",
    "#     ax[1].plot(preprocessed_signal, color='black')\n",
    "#     ax[1].set_title('rPPG Signal - After Filtering')\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(subject_path + f\"/{task}-filtered.png\")\n",
    "#     plt.close()\n",
    "            \n",
    "#     ## Compute the Frequency Component (After)\n",
    "#     # Perform FFT on the preprocessed RPPG signal\n",
    "#     fft_result = np.fft.fft(preprocessed_signal)\n",
    "#     fft_freqs = np.fft.fftfreq(len(preprocessed_signal), d=1/fs)\n",
    "\n",
    "#     # Take the magnitude of the FFT result\n",
    "#     fft_magnitude = np.abs(fft_result)\n",
    "\n",
    "#     # Plot the FFT result\n",
    "#     # Plot the FFT result\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(fft_freqs[:len(fft_freqs)//2], fft_magnitude[:len(fft_magnitude)//2], color='blue')\n",
    "#     plt.title('Frequency Spectrum of the RPPG Signal')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Magnitude')\n",
    "#     plt.grid(True)\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(subject_path + f\"/{task}-fft-after.png\")\n",
    "#     plt.close()\n",
    "    \n",
    "#     return preprocessed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.interpolate import interp1d\n",
    "# from scipy.signal import welch\n",
    "\n",
    "# def process_gt(subject_path, task, preprocessed_signal):\n",
    "\n",
    "#     ## Peaks diff\n",
    "#     peaks_diff = 0\n",
    "\n",
    "#     ## Load the Ground Truth\n",
    "#     gt_path = subject_path + f\"/bvp_{subject_path}_{task}.csv\"\n",
    "#     gt_ppg = pd.read_csv(gt_path, header=None).values\n",
    "\n",
    "#     ## Flatten the Signal\n",
    "#     gt_ppg = gt_ppg.flatten()\n",
    "\n",
    "#     ## Downsample to 30Hz\n",
    "#     original_fs = 64\n",
    "#     new_fs = 35\n",
    "\n",
    "#     ## Downsampling the GT PPG Signal\n",
    "#     gt_ppg = scipy.signal.resample(gt_ppg, int(len(gt_ppg) * new_fs / original_fs))\n",
    "\n",
    "#     ## Compute the Frequency Component (Before)\n",
    "#     # Perform FFT on the preprocessed RPPG signal\n",
    "#     fft_result = np.fft.fft(gt_ppg)\n",
    "#     fft_freqs = np.fft.fftfreq(len(gt_ppg), d=1/fs)\n",
    "\n",
    "#     # Take the magnitude of the FFT result\n",
    "#     fft_magnitude = np.abs(fft_result)\n",
    "\n",
    "#     # Plot the FFT result\n",
    "#     # Plot the FFT result\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(fft_freqs[:len(fft_freqs)//2], fft_magnitude[:len(fft_magnitude)//2], color='blue')\n",
    "#     plt.title('Frequency Spectrum of the GT Signal [Before]')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Magnitude')\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     ## Save the graph\n",
    "#     plt.savefig(subject_path + f\"/{task}-gt-fft-before.png\")\n",
    "#     plt.close()\n",
    "\n",
    "#     ## Preprocess the Ground Truth Signal\n",
    "#     gt_ppg = preprocess_ppg(gt_ppg, fs=35)\n",
    "    \n",
    "#     ## Compute the Frequency Component (After)\n",
    "#     # Perform FFT on the preprocessed RPPG signal\n",
    "#     fft_result = np.fft.fft(gt_ppg)\n",
    "#     fft_freqs = np.fft.fftfreq(len(gt_ppg), d=1/fs)\n",
    "\n",
    "#     # Take the magnitude of the FFT result\n",
    "#     fft_magnitude = np.abs(fft_result)\n",
    "\n",
    "#     # Plot the FFT result\n",
    "#     # Plot the FFT result\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(fft_freqs[:len(fft_freqs)//2], fft_magnitude[:len(fft_magnitude)//2], color='blue')\n",
    "#     plt.title('Frequency Spectrum of the GT Signal [After]')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Magnitude')\n",
    "#     plt.grid(True)\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(subject_path + f\"/{task}-gt-fft-after.png\")\n",
    "#     plt.close()\n",
    "\n",
    "#     ## Show the Filtered GT Signal\n",
    "#     plt.figure(figsize=(20, 5))\n",
    "#     plt.plot(gt_ppg, color='black')\n",
    "#     plt.title('GT Signal - Before Filtering')\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(subject_path + f\"/{task}-gt.png\")\n",
    "#     plt.close()\n",
    "\n",
    "#     ## Comparing the Signals\n",
    "#     ## Finds the Peaks from the Preprocessed Signal and Ground Truth\n",
    "#     min_distance = int((60 / 120) * 35)  # â‰ˆ  21 frames sample every peaks minimum(high stress)\n",
    "#     rppg_peaks, _  = scipy.signal.find_peaks(preprocessed_signal, distance=min_distance)\n",
    "#     gt_peaks, _ = scipy.signal.find_peaks(gt_ppg , distance=min_distance)\n",
    "\n",
    "#     ## If the differ of rppg and gt leng peaks are 5, let's add that\n",
    "#     peak_difference = abs(len(rppg_peaks) - len(gt_peaks))\n",
    "\n",
    "#     if peak_difference <= 5:\n",
    "#         peaks_diff += 1\n",
    "\n",
    "#     ## Print the Peaks\n",
    "#     print(f\"Number of Peaks in rPPG Signal: {len(rppg_peaks)}\")\n",
    "#     print(f\"Number of Peaks in Ground Truth: {len(gt_peaks)}\")   \n",
    "\n",
    "#     ## Plot signals comparison\n",
    "#     fig, ax = plt.subplots(2, 1, figsize=(20, 6))\n",
    "#     ax[0].plot(preprocessed_signal, color='black', label='rPPG Signal')\n",
    "#     ax[0].plot(rppg_peaks, preprocessed_signal[rppg_peaks], \"x\", color='red', label='rPPG Peaks')\n",
    "#     ax[0].set_title('rPPG Signal - Peaks')\n",
    "\n",
    "#     ## Plot text of number of HR\n",
    "#     ax[0].text(0.02 * len(preprocessed_signal), max(preprocessed_signal) * 0.9, f\"rPPG Peaks: {len(rppg_peaks)}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "#     ax[1].plot(gt_ppg, color='black', label='Ground Truth')\n",
    "#     ax[1].plot(gt_peaks, gt_ppg[gt_peaks], \"x\", color='green', label='Ground Truth Peaks')\n",
    "#     ax[1].set_title('Ground Truth - Peaks')\n",
    "\n",
    "#     ax[1].text(0.02 * len(gt_ppg), max(gt_ppg) * 0.9, f\"GT Peaks: {len(gt_peaks)}\", color='green', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(subject_path + f\"/{task}-comparison.png\")\n",
    "#     plt.close()\n",
    "\n",
    "#     ## Working to get the IBI / RR interval for both RPPG and GT\n",
    "#     rppg_peak_times = rppg_peaks / fs  # Convert to seconds\n",
    "#     gt_peak_times = gt_peaks / fs  # Convert to seconds\n",
    "#     rppg_peaks_diff = np.diff(rppg_peak_times) / fs  # Convert to seconds\n",
    "#     gt_peaks_diff = np.diff(gt_peak_times) / fs  # Convert to seconds\n",
    "\n",
    "#     ## Convert RR interval into ms\n",
    "#     rppg_peaks_diff_ms = rppg_peaks_diff * 1000  # Convert to milliseconds\n",
    "#     gt_peaks_diff_ms = gt_peaks_diff * 1000 # Convert into ms\n",
    "\n",
    "#     with open(f\"{subject_path}/{task}-rppg-hrv-value.txt\", \"w\") as f:\n",
    "#         # Mean IBI\n",
    "#         mean_rr = np.mean(rppg_peaks_diff_ms)\n",
    "\n",
    "#         # SDNN: Standard deviation of RR intervals\n",
    "#         sdnn = np.std(rppg_peaks_diff_ms)\n",
    "\n",
    "#         # RMSSD: Root mean square of successive differences\n",
    "#         rmssd = np.sqrt(np.mean(np.diff(rppg_peaks_diff_ms)**2))\n",
    "\n",
    "#         # pNN50: % of successive intervals that differ by more than 50 ms\n",
    "#         nn50 = np.sum(np.abs(np.diff(rppg_peaks_diff_ms)) > 50)\n",
    "#         pnn50 = 100 * nn50 / len(rppg_peaks_diff_ms)\n",
    "\n",
    "#         f.write(f\"Mean RR : {mean_rr:.2f} ,SDNN: {sdnn:.2f} ms, RMSSD: {rmssd:.2f} ms, pNN50: {pnn50:.2f}%\\n\")\n",
    "\n",
    "#         ## Calculate the Freq domain \n",
    "#         # Create uniform time axis for interpolation (e.g., 4Hz = every 0.25s)\n",
    "#         resample_rate = 4  # Hz\n",
    "#         uniform_time = np.arange(rppg_peak_times[1], rppg_peak_times[-1], 1 / resample_rate)\n",
    "\n",
    "#         ## Interpolate IBI Signals\n",
    "#         interp_func = interp1d(rppg_peak_times[1:], rppg_peaks_diff, kind=\"cubic\", fill_value=\"extrapolate\")\n",
    "#         ibi_signals = interp_func(uniform_time)\n",
    "\n",
    "#         ## Welch PSD\n",
    "#         freqs, psd = welch(ibi_signals, fs=resample_rate, nperseg=256)\n",
    "\n",
    "#         # LF = 0.04â€“0.15 Hz, HF = 0.15â€“0.4 Hz\n",
    "#         lf_band = (freqs >= 0.04) & (freqs < 0.15)\n",
    "#         hf_band = (freqs >= 0.15) & (freqs <= 0.4)\n",
    "\n",
    "#         lf_power = np.trapz(psd[lf_band], freqs[lf_band])\n",
    "#         hf_power = np.trapz(psd[hf_band], freqs[hf_band])\n",
    "#         lf_hf_ratio = lf_power / hf_power\n",
    "\n",
    "#         f.write(f\"LF: {lf_power:.4f}, HF: {hf_power:.4f}, LF/HF Ratio: {lf_hf_ratio:.2f}\\n\")\n",
    "\n",
    "\n",
    "#     with open(f\"{subject_path}/{task}-gt-hrv-value.txt\", \"w\") as f:\n",
    "#         # Mean IBI\n",
    "#         mean_rr = np.mean(gt_peaks_diff_ms)\n",
    "\n",
    "#         # SDNN: Standard deviation of RR intervals\n",
    "#         sdnn = np.std(gt_peaks_diff_ms)\n",
    "\n",
    "#         # RMSSD: Root mean square of successive differences\n",
    "#         rmssd = np.sqrt(np.mean(np.diff(gt_peaks_diff_ms)**2))\n",
    "\n",
    "#         # pNN50: % of successive intervals that differ by more than 50 ms\n",
    "#         nn50 = np.sum(np.abs(np.diff(gt_peaks_diff_ms)) > 50)\n",
    "#         pnn50 = 100 * nn50 / len(rppg_peaks_diff_ms)\n",
    "\n",
    "#         f.write(f\"Mean RR : {mean_rr:.2f} ,SDNN: {sdnn:.2f} ms, RMSSD: {rmssd:.2f} ms, pNN50: {pnn50:.2f}%\\n\")\n",
    "\n",
    "#         ## Calculate the Freq domain \n",
    "#         # Create uniform time axis for interpolation (e.g., 4Hz = every 0.25s)\n",
    "#         resample_rate = 4  # Hz\n",
    "#         uniform_time = np.arange(gt_peak_times[1], gt_peak_times[-1], 1 / resample_rate)\n",
    "\n",
    "#         ## Interpolate IBI Signals\n",
    "#         interp_func = interp1d(gt_peak_times[1:], gt_peaks_diff, kind=\"cubic\", fill_value=\"extrapolate\")\n",
    "#         ibi_signals = interp_func(uniform_time)\n",
    "\n",
    "#         ## Welch PSD\n",
    "#         freqs, psd = welch(ibi_signals, fs=resample_rate, nperseg=256)\n",
    "\n",
    "#         # LF = 0.04â€“0.15 Hz, HF = 0.15â€“0.4 Hz\n",
    "#         lf_band = (freqs >= 0.04) & (freqs < 0.15)\n",
    "#         hf_band = (freqs >= 0.15) & (freqs <= 0.4)\n",
    "\n",
    "#         lf_power = np.trapz(psd[lf_band], freqs[lf_band])\n",
    "#         hf_power = np.trapz(psd[hf_band], freqs[hf_band])\n",
    "#         lf_hf_ratio = lf_power / hf_power\n",
    "\n",
    "#         f.write(f\"LF: {lf_power:.4f}, HF: {hf_power:.4f}, LF/HF Ratio: {lf_hf_ratio:.2f}\\n\")\n",
    "\n",
    "\n",
    "#     return peaks_diff\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyrexian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
