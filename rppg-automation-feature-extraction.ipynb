{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Store the RPPG Result and the Graph of the Subject folders\n",
    "\n",
    "Will store the RPPG signal (raw) and then the pre-processed one (v0.1) and the image comparasion between the GT -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Dependencies\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import os\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=\"Mediapipe-Models/blaze_face_short_range.tflite\"\n",
    "# base_model=\"../Models/face_landmarker.task\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Define the POS method\n",
    "Pos Method by Wenjin Wang -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core method POS \n",
    "def POS(signal, **kargs):\n",
    "    \"\"\"\n",
    "    POS method on CPU using Numpy.\n",
    "\n",
    "    The dictionary parameters are: {'fps':float}.\n",
    "\n",
    "    Wang, W., den Brinker, A. C., Stuijk, S., & de Haan, G. (2016). Algorithmic principles of remote PPG. IEEE Transactions on Biomedical Engineering, 64(7), 1479-1491. \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    eps: A small constant (10^-9) used to prevent division by zero in normalization steps.\n",
    "    X: The input signal, which is a 3D array where:\n",
    "    e: Number of estimators or regions in the frame (like different parts of the face).\n",
    "    c: Color channels (3 for RGB).\n",
    "    f: Number of frames.\n",
    "    w: Window length, determined by the camera's frame rate (fps). For example, at 20 fps, w would be 32 (which corresponds to about 1.6 seconds of video).\n",
    "    \"\"\"\n",
    "    eps = 10**-9\n",
    "    X = signal\n",
    "    e, c, f = X.shape # Number of estimators, color channels, and frames\n",
    "    w = int(1.6 * kargs['fps']) # Window length in frames\n",
    "\n",
    "    \"\"\"\n",
    "    P: A fixed 2x3 matrix used for the projection step. It defines how to transform the color channels (RGB) into a new space.\n",
    "    Q: This is a stack of the matrix P repeated e times, where each P corresponds to an estimator (region of interest) in the video.\n",
    "    \"\"\"\n",
    "    P = np.array([[0, 1, -1], [-2, 1, 1]]) ## Pulse Rate Matricies\n",
    "    Q = np.stack([P for _ in range(e)], axis = 0)\n",
    "\n",
    "    \"\"\"\n",
    "    H: A matrix to store the estimated heart rate signal over time for each estimator.\n",
    "    n: The current frame in the sliding window.\n",
    "    m: The start index of the sliding window (calculating which frames are part of the current window).\n",
    "    \"\"\"\n",
    "    H = np.zeros((e, f))\n",
    "    for n in np.arange(w, f):\n",
    "        # Start index of sliding window \n",
    "        m = n - w + 1\n",
    "\n",
    "        \"\"\"\n",
    "        Temporal Normalization (Equation 5 from the paper): This step ensures that the signal is invariant to global lighting changes and other noise factors.\n",
    "        \"\"\"\n",
    "        Cn = X[:, :, m:(n+1)]\n",
    "        M = 1.0 / (np.mean(Cn, axis = 2) + eps)\n",
    "        M = np.expand_dims(M, axis=2) # shape [e, c, w]\n",
    "        Cn = np.multiply(Cn, M)\n",
    "\n",
    "        \"\"\"\n",
    "        Projection (Equation 6 from the paper): This step transforms the RGB values into a space where the signal from blood flow (heart rate) is more distinct.\n",
    "        \"\"\"\n",
    "        S = np.dot(Q, Cn)\n",
    "        S = S[0, :, :, :]\n",
    "        S = np.swapaxes(S, 0, 1) \n",
    "\n",
    "        \"\"\"\n",
    "        Tuning (Equation 7 from the paper): This step adjusts the projected components to make the heart rate signal clearer.\n",
    "        \"\"\"\n",
    "        S1 = S[:, 0, :]\n",
    "        S2 = S[:, 1, :]\n",
    "        alpha = np.std(S1, axis=1) / (eps + np.std(S2, axis=1))\n",
    "        alpha - np.expand_dims(alpha, axis=1)\n",
    "        Hn = np.add(S1, alpha * S2)\n",
    "        Hnm = Hn - np.expand_dims(np.mean(Hn, axis=1), axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        Overlap-Adding (Equation 8 from the paper): This step combines the processed signals from each frame to form the final output heart rate signal.\n",
    "        \"\"\"\n",
    "        H[:, m:(n + 1)] = np.add(H[:, m:(n + 1)], Hnm)  # Add the tuned signal to the output matrix\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CHROM(signal):\n",
    "    \"\"\"\n",
    "    CHROM method on CPU using Numpy.\n",
    "\n",
    "    De Haan, G., & Jeanne, V. (2013). Robust pulse rate from chrominance-based rPPG. \n",
    "    IEEE Transactions on Biomedical Engineering, 60(10), 2878-2886.\n",
    "    \"\"\"\n",
    "    X = signal\n",
    "    Xcomp = 3*X[:, 0] - 2*X[:, 1]\n",
    "    Ycomp = (1.5*X[:, 0])+X[:, 1]-(1.5*X[:, 2])\n",
    "    sX = np.std(Xcomp, axis=1)\n",
    "    sY = np.std(Ycomp, axis=1)\n",
    "    alpha = (sX/sY).reshape(-1, 1)\n",
    "    alpha = np.repeat(alpha, Xcomp.shape[1], 1)\n",
    "    bvp = Xcomp - np.multiply(alpha, Ycomp)\n",
    "    return bvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGI(signal):\n",
    "    \"\"\"\n",
    "    LGI method on CPU using Numpy.\n",
    "\n",
    "    Pilz, C. S., Zaunseder, S., Krajewski, J., & Blazek, V. (2018). Local group invariance for heart rate estimation from face videos in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 1254-1262).\n",
    "    \"\"\"\n",
    "    X = signal\n",
    "    U, _, _ = np.linalg.svd(X)\n",
    "    S = U[:, :, 0]\n",
    "    S = np.expand_dims(S, 2)\n",
    "    sst = np.matmul(S, np.swapaxes(S, 1, 2))\n",
    "    p = np.tile(np.identity(3), (S.shape[0], 1, 1))\n",
    "    P = p - sst\n",
    "    Y = np.matmul(P, X)\n",
    "    bvp = Y[:, 1, :]\n",
    "    return bvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_GREEN(signal):\n",
    "    \"\"\"\n",
    "    GREEN method on CPU using Numpy\n",
    "\n",
    "    Verkruysse, W., Svaasand, L. O., & Nelson, J. S. (2008). Remote plethysmographic imaging using ambient light. Optics express, 16(26), 21434-21445.\n",
    "    \"\"\"\n",
    "    return signal[:,1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_OMIT(signal):\n",
    "    \"\"\"\n",
    "    OMIT method on CPU using Numpy.\n",
    "\n",
    "    Álvarez Casado, C., Bordallo López, M. (2022). Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces. arXiv (eprint 2202.04101).\n",
    "    \"\"\"\n",
    "\n",
    "    bvp = []\n",
    "    for i in range(signal.shape[0]):\n",
    "        X = signal[i]\n",
    "        Q, R = np.linalg.qr(X)\n",
    "        S = Q[:, 0].reshape(1, -1)\n",
    "        P = np.identity(3) - np.matmul(S.T, S)\n",
    "        Y = np.dot(P, X)\n",
    "        bvp.append(Y[1, :])\n",
    "    bvp = np.array(bvp)\n",
    "    return bvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create faceDetector Object\n",
    "base_options = python.BaseOptions(model_asset_path=base_model)\n",
    "FaceDetector = mp.tasks.vision.FaceDetector\n",
    "FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "options = FaceDetectorOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode = VisionRunningMode.IMAGE,\n",
    ")\n",
    "detector = vision.FaceDetector.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create Facelandmarker Object\n",
    "# base_options = python.BaseOptions(model_asset_path=base_model)\n",
    "# VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "# options = vision.FaceLandmarkerOptions(\n",
    "#     base_options=base_options,\n",
    "#     num_faces=1,\n",
    "#     running_mode = VisionRunningMode.IMAGE,\n",
    "# )\n",
    "# detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cheek_rois(landmarks, image_shape):\n",
    "#     h, w, _ = image_shape\n",
    "#     left_cheek_indices = [111, 121, 50, 142]\n",
    "#     right_cheek_indices = [350, 340, 355, 280]\n",
    "\n",
    "#     left_cheek_points = [(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) for idx in left_cheek_indices]\n",
    "#     right_cheek_points = [(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) for idx in right_cheek_indices]\n",
    "\n",
    "#     left_cheek_rect = (\n",
    "#         min([pt[0] for pt in left_cheek_points]), min([pt[1] for pt in left_cheek_points]),\n",
    "#         max([pt[0] for pt in left_cheek_points]), max([pt[1] for pt in left_cheek_points])\n",
    "#     )\n",
    "#     # print(\"Left Cheek Rect:\", left_cheek_rect)\n",
    "#     right_cheek_rect = (\n",
    "#         min([pt[0] for pt in right_cheek_points]), min([pt[1] for pt in right_cheek_points]),\n",
    "#         max([pt[0] for pt in right_cheek_points]), max([pt[1] for pt in right_cheek_points])\n",
    "#     )\n",
    "#     # print(\"Right Cheek Rect:\", right_cheek_rect)\n",
    "\n",
    "#     return left_cheek_rect, right_cheek_rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_rgb_from_rect(rect, image):\n",
    "#     x_min, y_min, x_max, y_max = rect\n",
    "#     roi = image[y_min:y_max, x_min:x_max]\n",
    "#     return roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Preprocessing Signal -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_ppg(signal, fs = 30):\n",
    "#     \"\"\" Computes the Preprocessed PPG Signal, this steps include the following:\n",
    "#         1. Moving Average Smoothing\n",
    "#         2. Bandpass Filtering\n",
    "        \n",
    "#         Parameters:\n",
    "#         ----------\n",
    "#         signal (numpy array): \n",
    "#             The PPG Signal to be preprocessed\n",
    "#         fs (float): \n",
    "#             The Sampling Frequency of the Signal\n",
    "            \n",
    "#         Returns:\n",
    "#         --------\n",
    "#         numpy array: \n",
    "#             The Preprocessed PPG Signal\n",
    "    \n",
    "#     \"\"\" \n",
    "\n",
    "#     # Moving Average Smoothing\n",
    "#     window = int(fs * 0.15)  # 150ms window\n",
    "#     smoothed_signal = np.convolve(signal, np.ones(window)/window, mode='same')\n",
    "\n",
    "#     b, a = scipy.signal.butter(2, [0.5, 2.5], btype='band', fs=fs)\n",
    "#     filtered = scipy.signal.filtfilt(b, a, smoothed_signal)\n",
    "    \n",
    "#     # Additional lowpass to remove high-frequency noise\n",
    "#     b2, a2 = scipy.signal.butter(3, 2.5, btype='low', fs=fs)\n",
    "#     filtered = scipy.signal.filtfilt(b2, a2, filtered)\n",
    "    \n",
    "#     # Moving average smoothing\n",
    "#     window = int(fs * 0.15)  # 150ms window\n",
    "#     filtered_signal = np.convolve(filtered, np.ones(window)/window, mode='same')\n",
    "\n",
    "#     # Normalize the signal\n",
    "#     normalized_signal = (filtered_signal - np.min(filtered_signal)) / (np.max(filtered_signal) - np.min(filtered_signal))\n",
    "\n",
    "\n",
    "#     return normalized_signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## HRV - v0.1\n",
    "\n",
    "Extraction the RGB Signal over the entire frame image -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RGB Signal Variables\n",
    "\n",
    "    Variables to store the RGB signals from the left and right cheeks,\n",
    "    as well as the combined signal from both cheeks.\n",
    "\n",
    "\"\"\"\n",
    "## Utils\n",
    "fs = 30 # Sampling rate in Hz for RPPG signal\n",
    "\n",
    "\n",
    "# X should be increasing, y should be decreasing\n",
    "# w should be decreasing, h should be decreasing to fit the box into the face\n",
    "\n",
    "# Fixed adjustment values (in pixels)\n",
    "margin_x = 10  # Adjust horizontally\n",
    "scaling_factor = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Saving the RGB -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_roi_mediapipe(folders, subject_file):\n",
    "\n",
    "    # Lists to store combined RGB values\n",
    "    combined_r_signal, combined_g_signal, combined_b_signal = [], [], []\n",
    "\n",
    "    rgb_folder = folders + \"/rgb\"\n",
    "\n",
    "    # Load the video frames\n",
    "    video_files = sorted(os.listdir(rgb_folder))\n",
    "\n",
    "    # Load the face landmarks\n",
    "    for i in range(len(video_files) - 1): ## The last one is\n",
    "        # Load the image\n",
    "        image_path = os.path.join(rgb_folder, video_files[i])\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        ## Detect the face area using shape\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        ## Converting into RGB\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ## Seting up the Mediapipe Image\n",
    "        mp_image = mp.Image(\n",
    "            image_format=mp.ImageFormat.SRGB,\n",
    "            data=image_rgb\n",
    "        )\n",
    "\n",
    "        # ## Get the landkmarks\n",
    "        result = detector.detect(mp_image)\n",
    "\n",
    "        if result.detections:\n",
    "            for detection in result.detections:\n",
    "\n",
    "                ## Get the Bounding box\n",
    "                bboxC = detection.bounding_box\n",
    "                x, y, w, h = bboxC.origin_x, bboxC.origin_y, bboxC.width, bboxC.height\n",
    "\n",
    "                new_x = int(x + margin_x)\n",
    "\n",
    "                new_w = int(w * scaling_factor)\n",
    "                new_h = int(h * scaling_factor)\n",
    "\n",
    "                ## Get the ROI\n",
    "                face_roi = image_rgb[y:y+new_h, new_x:new_x+new_w]\n",
    "\n",
    "                ## Calculate the Mean\n",
    "                mean_rgb = cv2.mean(face_roi)[:3]\n",
    "                \n",
    "                # Append the combined RGB values to the respective lists\n",
    "                combined_r_signal.append(mean_rgb[0])\n",
    "                combined_g_signal.append(mean_rgb[1])\n",
    "                combined_b_signal.append(mean_rgb[2])\n",
    "\n",
    "        # Extract the face landmarks\n",
    "        # if result.face_landmarks:\n",
    "        #     for face_landmark in result.face_landmarks:\n",
    "        #         # Get cheek ROIs\n",
    "        #         left_cheek_rect, right_cheek_rect = get_cheek_rois(face_landmark, image_rgb.shape)\n",
    "\n",
    "        #         # Draw both cheek ROIs with rectangles\n",
    "        #         cv2.rectangle(image_rgb, (left_cheek_rect[0], left_cheek_rect[1]), (left_cheek_rect[2], left_cheek_rect[3]), (0, 255, 0), 2)\n",
    "        #         cv2.rectangle(image_rgb, (right_cheek_rect[0], right_cheek_rect[1]), (right_cheek_rect[2], right_cheek_rect[3]), (0, 255, 0), 2)\n",
    "\n",
    "        #         # Extract the left and right cheek ROIs\n",
    "        #         left_cheek_roi = extract_rgb_from_rect(left_cheek_rect, image_rgb)\n",
    "        #         right_cheek_roi = extract_rgb_from_rect(right_cheek_rect, image_rgb)\n",
    "\n",
    "        #         # Calculate mean pixel values for the RGB channels\n",
    "        #         left_cheek_rgb = cv2.mean(left_cheek_roi)[:3]\n",
    "        #         right_cheek_rgb = cv2.mean(right_cheek_roi)[:3]\n",
    "\n",
    "        #         # Combine and average the RGB values from both cheeks\n",
    "        #         combined_r = (left_cheek_rgb[0] + right_cheek_rgb[0]) / 2\n",
    "        #         combined_g = (left_cheek_rgb[1] + right_cheek_rgb[1]) / 2\n",
    "        #         combined_b = (left_cheek_rgb[2] + right_cheek_rgb[2]) / 2\n",
    "\n",
    "        #         # Append the combined RGB values to the respective lists\n",
    "        #         combined_r_signal.append(combined_r)\n",
    "        #         combined_g_signal.append(combined_g)\n",
    "        #         combined_b_signal.append(combined_b)\n",
    "\n",
    "\n",
    "    ## Calculating the RPPG signal\n",
    "    rgb_signals = np.array([combined_r_signal, combined_g_signal, combined_b_signal])\n",
    "    rgb_signals = rgb_signals.reshape(1, 3, -1)\n",
    "\n",
    "    ## POS\n",
    "    pos_signal = POS(rgb_signals, fps=30)\n",
    "    pos_signal = pos_signal.reshape(-1)\n",
    "\n",
    "    ## CHROM\n",
    "    chrom_signal = CHROM(rgb_signals)\n",
    "    chrom_signal = chrom_signal.reshape(-1)\n",
    "\n",
    "    ## LGI\n",
    "    lgi_signal = LGI(rgb_signals)\n",
    "    lgi_signal = lgi_signal.reshape(-1)\n",
    "\n",
    "    ## GREEN\n",
    "    green_signal = cpu_GREEN(rgb_signals)\n",
    "    green_signal = green_signal.reshape(-1)\n",
    "\n",
    "    ## OMIT\n",
    "    omit_signal = cpu_OMIT(rgb_signals)\n",
    "    omit_signal = omit_signal.reshape(-1)\n",
    "\n",
    "    ## Save the RPPG as .npy\n",
    "    pos_path = folders + f\"/{subject_file}-POS-rppg.npy\"\n",
    "    np.save(pos_path, pos_signal)\n",
    "\n",
    "    chrom_path = folders + f\"/{subject_file}-CHROM-rppg.npy\"\n",
    "    np.save(chrom_path, chrom_signal)\n",
    "\n",
    "    lgi_path = folders + f\"/{subject_file}-LGI-rppg.npy\"\n",
    "    np.save(lgi_path, lgi_signal)\n",
    "\n",
    "    green_path = folders + f\"/{subject_file}-GREEN-rppg.npy\"\n",
    "    np.save(green_path, green_signal)\n",
    "\n",
    "    omit_path = folders + f\"/{subject_file}-OMIT-rppg.npy\"\n",
    "    np.save(omit_path, omit_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physio-Itera/Dataset/ades2\n",
      "Physio-Itera/Dataset/ades6\n",
      "Physio-Itera/Dataset/adin2\n",
      "Physio-Itera/Dataset/adin6\n",
      "Physio-Itera/Dataset/agus2\n",
      "Physio-Itera/Dataset/agus6\n",
      "Physio-Itera/Dataset/aice2\n",
      "Physio-Itera/Dataset/aice6\n",
      "Physio-Itera/Dataset/alana2\n",
      "Physio-Itera/Dataset/alana6\n",
      "Physio-Itera/Dataset/alex2\n",
      "Physio-Itera/Dataset/alex6\n",
      "Physio-Itera/Dataset/ali2\n",
      "Physio-Itera/Dataset/ali6\n",
      "Physio-Itera/Dataset/alina2\n",
      "Physio-Itera/Dataset/alina6\n",
      "Physio-Itera/Dataset/anggur2\n",
      "Physio-Itera/Dataset/anggur6\n",
      "Physio-Itera/Dataset/ara2\n",
      "Physio-Itera/Dataset/ara6\n",
      "Physio-Itera/Dataset/arnold2\n",
      "Physio-Itera/Dataset/arnold6\n",
      "Physio-Itera/Dataset/bunny2\n",
      "Physio-Itera/Dataset/bunny6\n",
      "Physio-Itera/Dataset/cici2\n",
      "Physio-Itera/Dataset/cici6\n",
      "Physio-Itera/Dataset/citra2\n",
      "Physio-Itera/Dataset/citra6\n",
      "Physio-Itera/Dataset/dadu2\n",
      "Physio-Itera/Dataset/dadu6\n",
      "Physio-Itera/Dataset/dede2\n",
      "Physio-Itera/Dataset/dede6\n",
      "Physio-Itera/Dataset/deka2\n",
      "Physio-Itera/Dataset/deka6\n",
      "Physio-Itera/Dataset/fitsan2\n",
      "Physio-Itera/Dataset/fitsan6\n",
      "Physio-Itera/Dataset/fote2\n",
      "Physio-Itera/Dataset/fote6\n",
      "Physio-Itera/Dataset/gab2\n",
      "Physio-Itera/Dataset/gab6\n",
      "Physio-Itera/Dataset/tryx2\n",
      "Physio-Itera/Dataset/tryx6\n"
     ]
    }
   ],
   "source": [
    "## mapping the folders\n",
    "base_path=f\"Physio-Itera/Dataset\"\n",
    "\n",
    "## Mapping the folders\n",
    "folders=os.listdir(base_path)\n",
    "\n",
    "## Loop the folders /rgb and /vernier\n",
    "for folder in folders:\n",
    "    working_folder = base_path + f\"/{folder}\"\n",
    "    print(working_folder)\n",
    "    \n",
    "    ## Check if the folder is a directory\n",
    "    if os.path.isdir(working_folder):\n",
    "        ## Get the RGB and Vernier Folders as the path\n",
    "\n",
    "        subject = folder\n",
    "        rgb_folder = os.path.join(working_folder, \"rgb\")\n",
    "        vernier_folder = os.path.join(working_folder, \"vernier\")\n",
    "\n",
    "        ## Convert the RPPG \n",
    "        saving_roi_mediapipe(working_folder, f\"{subject}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_gt(folders, preprocessed_signal, image_file):\n",
    "#     target_folders = \"/processed\"\n",
    "\n",
    "#     ## Load the Ground Truth\n",
    "#     gt_path = folders + f\"/vernier/{image_file}_vernier_ecg.csv\"\n",
    "#     gt_ppg = pd.read_csv(gt_path, usecols=[1], header=None).values\n",
    "\n",
    "#     ## Flatten the Signal\n",
    "#     gt_ppg = gt_ppg.flatten()\n",
    "\n",
    "#     ## Downsample to 30Hz\n",
    "#     original_fs = 200\n",
    "#     new_fs = 30\n",
    "\n",
    "#     ## Downsampling the GT PPG Signal\n",
    "#     gt_ppg = scipy.signal.resample(gt_ppg, int(len(gt_ppg) * new_fs / original_fs))\n",
    "\n",
    "#     ## Preprocess the Ground Truth Signal\n",
    "#     gt_ppg = preprocess_ppg(gt_ppg, fs=30)\n",
    "\n",
    "#     ## Show the Filtered GT Signal\n",
    "#     plt.figure(figsize=(20, 5))\n",
    "#     plt.plot(gt_ppg, color='black')\n",
    "#     plt.title('GT Signal - Before Filtering')\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(folders + target_folders + f\"/{image_file}-gt.png\")\n",
    "#     plt.close()\n",
    "\n",
    "#     ## Comparing the Signals\n",
    "#     ## Finds the Peaks from the Preprocessed Signal and Ground Truth\n",
    "#     rppg_peaks, _  = scipy.signal.find_peaks(preprocessed_signal)\n",
    "#     gt_peaks, _ = scipy.signal.find_peaks(gt_ppg)\n",
    "\n",
    "#     ## Print the Peaks\n",
    "#     print(f\"Number of Peaks in rPPG Signal: {len(rppg_peaks)}\")\n",
    "#     print(f\"Number of Peaks in Ground Truth: {len(gt_peaks)}\")   \n",
    "\n",
    "#     ## Plot the Signal\n",
    "#     plt.figure(figsize=(20, 5)) \n",
    "#     plt.plot(preprocessed_signal, color='black', label='rPPG Signal')\n",
    "#     plt.plot(rppg_peaks, preprocessed_signal[rppg_peaks], \"x\", color='red', label='rPPG Peaks')\n",
    "#     plt.plot(gt_ppg, color='blue', label='Ground Truth')\n",
    "#     plt.plot(gt_peaks, gt_ppg[gt_peaks], \"x\", color='green', label='Ground Truth Peaks')\n",
    "\n",
    "#     ## Adjust the text position dynamically\n",
    "#     y_max = max(max(preprocessed_signal), max(gt_ppg))  # Get the maximum y-value from both signals\n",
    "#     plt.text(0.02 * len(preprocessed_signal), y_max * 0.9, \n",
    "#             f\"rPPG Peaks: {len(rppg_peaks)}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "#     plt.text(0.02 * len(preprocessed_signal), y_max * 0.8, \n",
    "#             f\"GT Peaks: {len(gt_peaks)}\", color='green', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "#     ## Add title, labels, and legend\n",
    "#     plt.title('rPPG Signal vs Ground Truth')\n",
    "#     plt.xlabel(\"Samples\")\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.legend(['rPPG Signal', 'rPPG Peaks', 'Ground Truth', 'Ground Truth Peaks'])\n",
    "    \n",
    "#     ## Save the graph\n",
    "#     plt.savefig(folders + target_folders + f\"/{image_file}-comparison.png\")\n",
    "#     plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codex_astartes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
